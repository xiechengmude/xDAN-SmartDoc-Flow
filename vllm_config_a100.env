# vLLM Server Configuration for A100 40GB
# Optimized settings for maximum performance

# Model settings
MODEL_PATH=ChatDOC/OCRFlux-3B
PORT=8003

# A100 40GB optimized memory and GPU settings
MAX_MODEL_LEN=8192                # Keep at 8192 as requested
GPU_MEMORY_UTILIZATION=0.95       # Use 95% of 40GB = ~38GB
TENSOR_PARALLEL_SIZE=1            # Single A100

# High-performance batch processing settings
MAX_NUM_SEQS=10                   # Limited to 10 concurrent requests
MAX_NUM_BATCHED_TOKENS=32768      # 4x increase from default 8192

# A100 specific optimizations
DTYPE=float16                     # FP16 for A100 tensor cores
BLOCK_SIZE=16                     # Optimal for A100
ENABLE_PREFIX_CACHING=true        # Cache repeated prefixes
USE_V2_BLOCK_MANAGER=true         # Better memory management

# Performance estimates with these settings:
# - Can handle ~1000+ concurrent requests
# - ~38GB VRAM usage (out of 40GB)
# - Leaves 2GB for safety margin
# - Optimal for OCRFlux-3B model size (~6-8GB)

# API server settings
API_HOST=0.0.0.0
API_PORT=8000
VLLM_URL=http://localhost
VLLM_PORT=8003
DEBUG=false