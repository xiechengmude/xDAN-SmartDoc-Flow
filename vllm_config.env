# vLLM Server Configuration
# Copy this file to .env and adjust values as needed

# Model settings
MODEL_PATH=ChatDOC/OCRFlux-3B
PORT=30024

# Memory and GPU settings
MAX_MODEL_LEN=8192                # Maximum sequence length
GPU_MEMORY_UTILIZATION=0.8        # Fraction of GPU memory to use (0.0-1.0)
TENSOR_PARALLEL_SIZE=1            # Number of GPUs for tensor parallelism

# Batch processing settings
MAX_NUM_SEQS=256                  # Maximum number of sequences per iteration
MAX_NUM_BATCHED_TOKENS=8192       # Maximum number of tokens per batch

# Performance optimizations
DTYPE=auto                        # Data type: auto, float16, bfloat16, float32
QUANTIZATION=                     # Options: awq, gptq, squeezellm, None
TRUST_REMOTE_CODE=true           # Trust remote code from HuggingFace

# API server settings (for api_server.py)
API_HOST=0.0.0.0
API_PORT=8000
VLLM_URL=http://localhost
VLLM_PORT=30024
DEBUG=false